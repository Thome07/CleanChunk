# bom, porem tem Respostas que estao vindo muito grandes


from tkinter import *
from tkinter import ttk, filedialog, messagebox
from sentence_transformers import SentenceTransformer
import numpy as np
import re
import os
import json
import pdfplumber
import docx
from unidecode import unidecode
import ftfy
import logging
from typing import List, Dict, Any, Optional
import threading
from queue import Queue
import google.generativeai as genai
import time
import requests
from datetime import datetime, timedelta

try:
    from transformers import pipeline, AutoTokenizer, AutoModel
    import torch
    from sklearn.cluster import KMeans
    from sklearn.metrics.pairwise import cosine_similarity
    import umap
    semantic_analyzer = pipeline("text-classification", model="microsoft/DialoGPT-medium")
    coherence_checker = pipeline("text-classification", model="textattack/bert-base-uncased-CoLA")
    ADVANCED_NLP = True
except ImportError:
    ADVANCED_NLP = False
    print("AVISO: Bibliotecas avan√ßadas n√£o instaladas. Usando fallback.")

class DocumentProcessor:
    def __init__(self, gemini_api_key: str = None):
        # Modelo mais leve e r√°pido, mas ainda eficiente
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.model.max_seq_length = 256
        
        # Configura√ß√£o Gemini
        self.gemini_api_key = gemini_api_key
        self.gemini_model = None
        self.use_gemini = False
        self.request_count = 0
        self.last_request_time = datetime.now()
        
        # Par√¢metros otimizados para chunks SEM√ÇNTICOS (n√£o fixos!)
        self.min_chunk_words = 40      # M√≠nimo em palavras
        self.max_chunk_words = 150     # M√°ximo em palavras  
        self.ideal_chunk_words = 80    # Ideal em palavras
        self.min_sentences_per_chunk = 2
        self.max_sentences_per_chunk = 8
        
        # Rate limiting otimizado
        self.max_requests_per_minute = 14  # Margem de seguran√ßa
        self.request_interval = 4.0        # 4.0 segundos entre requests
        self.batch_size_gemini = 15         # Processa 15 senten√ßas por vez com Gemini
        
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                self.gemini_model = genai.GenerativeModel('gemini-1.5-flash')
                self.use_gemini = True
                print("‚úÖ Gemini AI configurado com sucesso!")
            except Exception as e:
                print(f"‚ùå Erro ao configurar Gemini: {e}")
                self.use_gemini = False
        
        # Threshold sem√¢ntico mais flex√≠vel
        self.threshold_base = 0.75
        self.threshold_flexible = 0.65  # Para chunks pequenos
        self.batch_size = 32
        
        # Cache para otimiza√ß√£o
        self._validation_cache = {}
        self._embedding_cache = {}
        self._semantic_cache = {}
        
        # Modelo maior para melhor qualidade sem√¢ntica
        self.model_semantica = SentenceTransformer('all-MiniLM-L12-v2')  # Modelo mais preciso
        self.threshold_semantica = 0.85  # Threshold mais alto para maior precis√£o
        
        # Par√¢metros otimizados para sem√¢ntica perfeita
        self.min_words = 15  # M√≠nimo em palavras, n√£o caracteres
        self.max_words = 80  # M√°ximo em palavras (60-120 como sugerido)
        self.ideal_words = 50  # Tamanho ideal
        self.min_sentences = 2  # M√≠nimo de frases por chunk
        self.max_sentences = 6  # M√°ximo de frases por chunk
        
        # Regex melhorados
        self.sentence_splitter = re.compile(r'(?<=[.!?])\s+(?=[A-Z])')
        self.paragraph_splitter = re.compile(r'\n\s*\n')
        
        # Padr√µes para detectar ideias completas
        self.idea_starters = re.compile(r'\b(?:Portanto|Assim|Dessa forma|Por isso|Consequentemente|Em suma|Al√©m disso|Tamb√©m|Primeiro|Segundo|Finalmente)\b', re.IGNORECASE)
        self.idea_enders = re.compile(r'\b(?:concluindo|resumindo|em conclus√£o|por fim|finalmente)\b', re.IGNORECASE)

        # Regex patterns necess√°rios
        self.page_re = re.compile(r'\[P√°gina\s+(\d+)\]')
        self.header_re = re.compile(r'^[A-Z\s]{5,}$')
        self.faq_q_re = re.compile(r'^\d+\.\s*(.+)$')
        self.faq_a_re = re.compile(r'^R:\s*(.+)$')
        self.header_interview_re = re.compile(r'^[A-Z\s]{5,}$')
        self.speaker_re = re.compile(r'^([^:]+):\s*(.+)$')

        self._gemini_cache = {}

    def tem_sentido(self, texto: str) -> bool:
        """Verifica se o texto tem sentido usando modelo de gram√°tica"""
        if not coherence_checker or len(texto) < 3:
            return len(texto) >= 10  # Fallback mais permissivo
        try:
            resultado = coherence_checker(texto[:512])
            return resultado[0]['label'] == 'LABEL_1' and resultado[0]['score'] > 0.6  # Threshold mais baixo
        except Exception:
            return len(texto) >= 10

    def limpar_texto(self, texto: str) -> str:
        """Limpeza otimizada de texto"""
        if not texto:
            return ""
        
        # Cache para textos j√° limpos
        if texto in self._validation_cache:
            return self._validation_cache[texto]
        
        # Fix encoding issues
        texto = ftfy.fix_text(texto)
        
        # Limpeza em uma passada usando regex
        replacements = [
            (r'\\+["\']', '"'),
            (r'-\s*\n', ''),
            (r'\r\n|\r|\n', ' '),
            (r'\s+', ' '),
            (r'\s+([,\.!?;:])', r'\1'),
            (r'\\(["\'\\])', r'\1'),
            (r'\\n', ' '),
            (r'\u201c|\u201d|&quot;', '"'),
            (r'\u2018|\u2019|&#39;', "'"),
        ]
        
        for pattern, replacement in replacements:
            texto = re.sub(pattern, replacement, texto)
        
        # Cache do resultado
        result = texto.strip()
        self._validation_cache[texto] = result
        return result

    def preprocess_text(self, text: str) -> str:
        """Preprocessamento espec√≠fico para limpeza de ru√≠do"""
        if not text:
            return ""
            
        # Remove linhas num√©ricas isoladas
        text = re.sub(r'^\s*\d+\.?\s*$', '', text, flags=re.MULTILINE)
        # Remove palavras em CAPS muito curtas
        text = re.sub(r'\b[A-Z]{1,4}\b\.?', '', text)
        # Remove pontua√ß√£o repetida
        text = re.sub(r'([.!?])\1+', r'\1', text)
        
        return text.strip()

    def load_document(self, path: str) -> str:
        """Carrega documento de diferentes formatos"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {path}")
            
        ext = os.path.splitext(path)[1].lower()
        content = ""
        
        try:
            if ext == '.pdf':
                with pdfplumber.open(path) as pdf:
                    parts = []
                    for i, page in enumerate(pdf.pages, 1):
                        raw = page.extract_text()
                        if raw:
                            raw = raw.replace('\\"', '"').replace("\\'", "'")
                        if raw.strip():  # S√≥ adiciona se tem conte√∫do
                            parts.append(f"[P√°gina {i}]\n{raw}")
                    content = "\n\n".join(parts)
            elif ext == '.docx':
                doc = docx.Document(path)
                parts = [para.text for para in doc.paragraphs if para.text.strip()]
                content = "\n\n".join(parts)
            else:  # .txt
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
        except Exception as e:
            raise Exception(f"Erro ao ler arquivo: {str(e)}")
        
        return content

    def load_lines(self, path: str, is_faq_mode: bool = False) -> List[str]: # Adicionado is_faq_mode
        """Carrega documento preservando linhas, com tratamento especial para modo FAQ."""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Arquivo n√£o encontrado: {path}")
            
        ext = os.path.splitext(path)[1].lower()
        lines = []
        
        try:
            if ext == '.pdf':
                with pdfplumber.open(path) as pdf:
                    for i, page in enumerate(pdf.pages, 1):
                        raw = page.extract_text(layout=True) 
                        if raw:
                            raw = raw.replace('-\n', '')  # Remove hifens de quebra de linha

                            if is_faq_mode:
                                # Para FAQ, mantenha as linhas individuais da p√°gina
                                page_actual_lines = [l.strip() for l in raw.splitlines() if l.strip()]
                                lines.extend(page_actual_lines)
                            else:
                                # Comportamento original para outros modos (n√£o FAQ)
                                raw = raw.replace('\n', ' ') # Substitui quebras por espa√ßos
                                if raw.strip(): # S√≥ adiciona se tem conte√∫do
                                    lines.append(f"[P√°gina {i}]")
                                    # Ap√≥s replace('\n', ' '), raw.splitlines() provavelmente ter√° 1 item ou poucos
                                    lines.extend([l.strip() for l in raw.splitlines() if l.strip()])
            elif ext == '.docx':
                doc = docx.Document(path)
                if is_faq_mode:
                    for para in doc.paragraphs:
                        if para.text.strip(): # Checa se o par√°grafo tem algum conte√∫do
                            # Para FAQ, divide o par√°grafo por quebras de linha internas
                            paragraph_lines = [line.strip() for line in para.text.splitlines() if line.strip()]
                            lines.extend(paragraph_lines)
                else:
                    # Comportamento original para n√£o-FAQ: cada par√°grafo n√£o vazio √© uma linha
                    for para in doc.paragraphs:
                        if para.text.strip():
                             lines.append(para.text) # O original mantinha o texto do par√°grafo
            elif ext == '.txt': # .txt
                with open(path, 'r', encoding='utf-8') as f:
                    # Para TXT, o comportamento de preservar linhas j√° √© adequado para FAQ
                    lines = [l.strip() for l in f if l.strip()] # strip() em vez de rstrip('\n') para remover espa√ßos em branco das extremidades
            else:
                 # Caso para outros tipos de arquivo ou se um comportamento padr√£o for necess√°rio
                 # Voc√™ pode querer levantar um erro para tipos n√£o suportados ou ter um fallback
                with open(path, 'r', encoding='utf-8') as f:
                    lines = [l.strip() for l in f if l.strip()]

        except Exception as e:
            # Adiciona o modo FAQ √† mensagem de erro se aplic√°vel
            error_prefix = f"Erro ao ler arquivo para {'FAQ' if is_faq_mode else 'processamento normal'}: "
            raise Exception(error_prefix + str(e))
        
        return lines
    
    def split_sentences(self, text: str) -> List[str]:
        """Divide texto em senten√ßas"""
        if not text:
            return []
        # Regex melhorado para lidar com diferentes casos de pontua√ß√£o
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z√Ä-√ú])', text)  # Adicionado suporte para caracteres acentuados
        return [s.strip() for s in sentences if s.strip()]

    def _is_chunk_valid(self, chunk_sentences: List[str]) -> bool:
        """Valida se um chunk tem conte√∫do sem√¢ntico v√°lido"""
        if not chunk_sentences:
            return False
            
        chunk_text = ' '.join(chunk_sentences).strip()
        
        # Crit√©rios b√°sicos de tamanho
        if len(chunk_text) < self.min_words:
            return False
        
        # Verifica se tem palavras suficientes (n√£o s√≥ pontua√ß√£o/n√∫meros)
        words = re.findall(r'\b\w+\b', chunk_text)
        if len(words) < 10:  # Pelo menos 10 palavras
            return False
        
        # Verifica se n√£o termina no meio de uma frase
        if not chunk_text.rstrip().endswith(('.', '!', '?', ':', ';')):
            # Se n√£o termina com pontua√ß√£o, verifica se a √∫ltima senten√ßa est√° completa
            last_sentence = chunk_sentences[-1].strip()
            if len(last_sentence) < 20 or not self._sentence_seems_complete(last_sentence):
                return False
        
        # Verifica se n√£o tem muitos caracteres estranhos ou fragmenta√ß√£o
        clean_chars = re.sub(r'[^\w\s]', '', chunk_text)
        if len(clean_chars) < len(chunk_text) * 0.7:  # Muito s√≠mbolo/pontua√ß√£o
            return False
        
        # Verifica diversidade vocabular b√°sica
        unique_words = set(word.lower() for word in words if len(word) > 2)
        if len(unique_words) < len(words) * 0.4:  # Muito repetitivo
            return False
        
        # Verifica se n√£o √© s√≥ n√∫meros ou c√≥digos
        if re.match(r'^[\d\s\-\.]+$', chunk_text):
            return False
            
        return True

    def _sentence_seems_complete(self, sentence: str) -> bool:
        """Verifica se uma senten√ßa parece completa (m√©todo j√° estava sendo chamado)"""
        if not sentence:
            return False
        
        sentence = sentence.strip()
        
        # Muito curta
        if len(sentence) < 15:
            return False
        
        # Termina com pontua√ß√£o final
        if sentence.endswith(('.', '!', '?')):
            return True
        
        # Termina com dois pontos ou ponto-e-v√≠rgula (pode ser v√°lido)
        if sentence.endswith((':', ';')):
            return len(sentence) > 25  # S√≥ aceita se for longa o suficiente
        
        # Verifica se n√£o termina com conectivos ou preposi√ß√µes
        incomplete_endings = [
            r'\b(e|mas|que|quando|se|porque|para|com|em|de|da|do|na|no|pela|pelo|sobre|entre|durante|antes|depois)\s*$',
            r'\b(o|a|os|as|um|uma|uns|umas)\s*$',  # Artigos soltos
            r'\b(muito|mais|menos|bem|mal|melhor|pior)\s*$',  # Adv√©rbios incompletos
        ]
        
        for pattern in incomplete_endings:
            if re.search(pattern, sentence, re.IGNORECASE):
                return False
        
        # Se chegou at√© aqui e tem tamanho razo√°vel, provavelmente est√° completa
        return len(sentence) >= 20

    def _clean_chunk_text(self, text: str) -> str:
        """Vers√£o otimizada da limpeza de texto para chunks"""
        if not text:
            return ""
        
        # Cache para evitar processamento repetido
        cache_key = hash(text[:100])  # Use primeiros 100 chars como chave
        if cache_key in self._validation_cache:
            cached_result = self._validation_cache.get(f"clean_{cache_key}")
            if cached_result is not None:
                return cached_result
        
        # Limpeza otimizada em uma passada
        # Remove quebras de linha e normaliza espa√ßos
        text = re.sub(r'\s+', ' ', text)
        
        # Remove caracteres de escape problem√°ticos
        text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')
        text = re.sub(r'\\+', '', text)
        
        # Corrige pontua√ß√£o
        text = re.sub(r'([.!?])\s*\1+', r'\1', text)  # Remove pontua√ß√£o duplicada
        text = re.sub(r'\s+([,.!?;:])', r'\1', text)  # Remove espa√ßos antes de pontua√ß√£o
        text = re.sub(r'([,.!?;:])\s*([,.!?;:])', r'\1 \2', text)  # Espa√ßa pontua√ß√£o m√∫ltipla
        
        # Limpa in√≠cio e fim
        text = re.sub(r'^[^\w\s]*', '', text)  # Remove s√≠mbolos no in√≠cio
        text = re.sub(r'[^\w\s.!?]*$', '', text)  # Remove s√≠mbolos no fim (exceto pontua√ß√£o)
        
        # Garante que termine com pontua√ß√£o se n√£o terminar
        text = text.strip()
        if text and not text[-1] in '.!?:;':
            # S√≥ adiciona ponto se a √∫ltima palavra parecer completa
            words = text.split()
            if words and len(words[-1]) >= 3:
                text += '.'
        
        # Cache do resultado
        self._validation_cache[f"clean_{cache_key}"] = text
        
        return text

    def _rate_limit_gemini(self):
        """Rate limiting otimizado e n√£o-bloqueante"""
        now = datetime.now()
        time_diff = (now - self.last_request_time).total_seconds()
        
        # Reset contador a cada minuto
        if time_diff >= 60:
            self.request_count = 0
            self.last_request_time = now
        
        # Se atingiu limite, calcula tempo de espera preciso
        if self.request_count >= self.max_requests_per_minute:
            wait_time = 60 - time_diff
            if wait_time > 0:
                print(f"‚è≥ Rate limit: aguardando {wait_time:.1f}s...")
                time.sleep(wait_time + 0.5)  # +0.5s margem
                self.request_count = 0
                self.last_request_time = datetime.now()
        
        # Intervalo m√≠nimo otimizado
        elif time_diff < self.request_interval:
            sleep_time = self.request_interval - time_diff
            time.sleep(sleep_time)

    def _gemini_analyze_semantic_break_batch(self, sentences: List[str]) -> List[bool]:
        cache_key = hash(tuple(sentences))
        if cache_key in self._gemini_cache:
            return self._gemini_cache[cache_key]
        if not self.use_gemini or len(sentences) < 2:
            return [False] * (len(sentences) - 1)
        try:
            self._rate_limit_gemini()
            # Prepara texto das senten√ßas
            sentence_pairs = []
            for i in range(len(sentences) - 1):
                s1 = sentences[i][:150]  # Limita tamanho
                s2 = sentences[i + 1][:150]
                sentence_pairs.append(f"{i+1}. \"{s1}\" ‚Üí \"{s2}\"")
            max_pairs = 20  # Aumente o n√∫mero de pares analisados por request
            if len(sentence_pairs) > max_pairs:
                result = self._gemini_analyze_semantic_break_batch(sentences[:max_pairs+1]) + \
                         self._gemini_analyze_semantic_break_batch(sentences[max_pairs:])
                self._gemini_cache[cache_key] = result
                return result
            pairs_text = "\n".join(sentence_pairs)
            prompt = f"""Analise as transi√ß√µes entre estas {len(sentence_pairs)} senten√ßas consecutivas.\nPara cada par, responda 1 (QUEBRA) ou 0 (CONTINUA) separados por v√≠rgula:\n\n{pairs_text}\n\nResposta:"""
            response = self.gemini_model.generate_content(prompt)
            self.request_count += 1
            # Parse da resposta
            breaks_text = response.text.strip()
            breaks = []
            for x in breaks_text.split(','):
                try:
                    breaks.append(int(x.strip()) == 1)
                except:
                    breaks.append(False)
            # Ajusta tamanho se necess√°rio
            expected_len = len(sentences) - 1
            if len(breaks) != expected_len:
                breaks = breaks[:expected_len] + [False] * (expected_len - len(breaks))
            self._gemini_cache[cache_key] = breaks
            return breaks
        except Exception as e:
            print(f"‚ö†Ô∏è Erro Gemini batch: {e}")
            return [False] * (len(sentences) - 1)

    def chunk_semantic_pairwise(self, sentences: List[str], pagina: Optional[int] = None) -> List[Dict]:
        """Chunking sem√¢ntico ADAPTATIVO com foco na qualidade sem√¢ntica"""
        if not sentences:
            return []
        
        # Filtra e limpa senten√ßas
        valid_sentences = []
        for sentence in sentences:
            cleaned = self.limpar_texto(sentence)
            if len(cleaned) >= 15 and len(re.findall(r'\b\w+\b', cleaned)) >= 3:
                valid_sentences.append(cleaned)
        
        if len(valid_sentences) < 2:
            return []
        
        chunks = []
        print(f"üìù Processando {len(valid_sentences)} senten√ßas...")
        
        if self.use_gemini:
            print("ü§ñ Usando Gemini AI para an√°lise sem√¢ntica...")
            chunks = self._chunk_with_gemini_semantic(valid_sentences, pagina)
        else:
            print("üìä Usando embeddings para an√°lise sem√¢ntica...")
            chunks = self._chunk_with_embeddings_semantic(valid_sentences, pagina)
        
        print(f"‚úÖ Gerados {len(chunks)} chunks sem√¢nticos")
        return chunks

    def _chunk_with_gemini_semantic(self, sentences: List[str], pagina: Optional[int] = None) -> List[Dict]:
        """Chunking com Gemini focado na sem√¢ntica"""
        chunks = []
        current_chunk = []
        
        # Processa em lotes menores para efici√™ncia
        batch_size = self.batch_size_gemini
        i = 0
        
        while i < len(sentences):
            # Pega lote atual
            end_idx = min(i + batch_size, len(sentences))
            batch_sentences = sentences[i:end_idx]
            
            # Analisa quebras sem√¢nticas do lote
            if len(batch_sentences) > 1:
                semantic_breaks = self._gemini_analyze_semantic_break_batch(batch_sentences)
            else:
                semantic_breaks = []
            
            # Processa senten√ßas do lote
            for j, sentence in enumerate(batch_sentences):
                current_chunk.append(sentence)
                word_count = self._count_words_in_chunk(current_chunk)
                
                should_break = False
                
                # Crit√©rios para quebra
                if word_count >= self.max_chunk_words:
                    should_break = True
                elif (word_count >= self.ideal_chunk_words and 
                      len(current_chunk) >= self.min_sentences_per_chunk):
                    # Verifica se Gemini sugere quebra sem√¢ntica
                    if j < len(semantic_breaks) and semantic_breaks[j]:
                        should_break = True
                elif len(current_chunk) >= self.max_sentences_per_chunk:
                    should_break = True
                
                # Cria chunk se deve quebrar
                if should_break and len(current_chunk) > 1:
                    # Remove √∫ltima senten√ßa para o pr√≥ximo chunk
                    chunk_sentences = current_chunk[:-1]
                    if self._is_valid_semantic_chunk_words(chunk_sentences):
                        chunk_text = self._clean_chunk_text_optimized(' '.join(chunk_sentences))
                        if chunk_text:
                            chunks.append({
                                'pagina': pagina,
                                'conteudo': chunk_text,
                                'tipo_quebra': 'semantica_gemini',
                                'confianca': 0.92,
                                'palavras': len(chunk_text.split()),
                                'sentencas': len(chunk_sentences)
                            })
                    
                    # Inicia novo chunk com a senten√ßa atual
                    current_chunk = [sentence]
            
            i = end_idx
            print(f"üìä Processado lote {i//batch_size}/{(len(sentences) + batch_size - 1)//batch_size}")
        
        # Processa √∫ltimo chunk
        if current_chunk and self._is_valid_semantic_chunk_words(current_chunk):
            chunk_text = self._clean_chunk_text_optimized(' '.join(current_chunk))
            if chunk_text:
                chunks.append({
                    'pagina': pagina,
                    'conteudo': chunk_text,
                    'tipo_quebra': 'final',
                    'confianca': 0.90,
                    'palavras': len(chunk_text.split()),
                    'sentencas': len(current_chunk)
                })
        
        return chunks

    def _chunk_with_embeddings_semantic(self, sentences: List[str], pagina: Optional[int] = None) -> List[Dict]:
        """Chunking com embeddings aprimorado com an√°lise de coes√£o."""
        if not sentences:
            return []
            
        chunks = []
        current_chunk_sentences = []
        similarities = self._calculate_semantic_similarity_batch(sentences)
        
        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            current_chunk_sentences.append(sentence)
            
            word_count = self._count_words_in_chunk(current_chunk_sentences)
            num_sentences = len(current_chunk_sentences)
            
            # Verifica se √© a √∫ltima senten√ßa do par√°grafo
            is_last_sentence = (i == len(sentences) - 1)
            
            # Define o limiar de similaridade para a quebra.
            # Um valor mais baixo significa que a pr√≥xima frase tem que ser bem diferente para causar uma quebra.
            similarity_threshold = 0.5 

            # Condi√ß√£o de quebra: a pr√≥xima frase tem baixa similaridade E o chunk atual j√° tem um tamanho razo√°vel
            break_due_to_similarity = (i < len(similarities) and 
                                       similarities[i] < similarity_threshold and 
                                       word_count >= self.min_chunk_words)

            # Condi√ß√£o de quebra: o chunk est√° ficando muito grande
            break_due_to_max_size = word_count >= self.max_chunk_words

            # Se for a √∫ltima senten√ßa ou se uma condi√ß√£o de quebra foi atingida
            if is_last_sentence or break_due_to_similarity or break_due_to_max_size:
                
                # Valida√ß√£o final usando coes√£o
                cohesion_score = self._calculate_chunk_cohesion(current_chunk_sentences)
                
                # S√≥ aceita o chunk se ele for coeso ou se for o final do par√°grafo
                if cohesion_score > 0.7 or is_last_sentence:
                    chunk_text = self._clean_chunk_text_optimized(' '.join(current_chunk_sentences))
                    if self._is_valid_semantic_chunk_words(current_chunk_sentences):
                        chunks.append({
                            'pagina': pagina,
                            'conteudo': chunk_text,
                            'tipo_quebra': 'semantica_coesao' if not is_last_sentence else 'final_paragrafo',
                            'confianca': float(cohesion_score),
                            'palavras': len(chunk_text.split()),
                            'sentencas': len(current_chunk_sentences)
                        })
                    current_chunk_sentences = [] # Limpa para o pr√≥ximo chunk
                # Se n√£o for coeso, tenta adicionar a pr√≥xima frase para ver se melhora a coes√£o
            
            i += 1
            
        # Garante que qualquer resto no buffer seja processado
        if current_chunk_sentences:
            chunk_text = self._clean_chunk_text_optimized(' '.join(current_chunk_sentences))
            if self._is_valid_semantic_chunk_words(current_chunk_sentences):
                chunks.append({
                    'pagina': pagina,
                    'conteudo': chunk_text,
                    'tipo_quebra': 'final',
                    'confianca': 0.85,
                    'palavras': len(chunk_text.split()),
                    'sentencas': len(current_chunk_sentences)
                })

        return chunks

    def _count_words_in_chunk(self, sentences: List[str]) -> int:
        """Conta palavras no chunk atual"""
        text = ' '.join(sentences)
        return len(re.findall(r'\b\w+\b', text))

    def _is_valid_semantic_chunk_words(self, sentences: List[str]) -> bool:
        """Valida chunk baseado em contagem de palavras"""
        if not sentences:
            return False
        
        word_count = self._count_words_in_chunk(sentences)
        
        # Crit√©rios baseados em palavras, n√£o caracteres
        if word_count < self.min_chunk_words:
            return False
        
        text = ' '.join(sentences)
        
        # Verifica se termina adequadamente
        if not text.rstrip().endswith(('.', '!', '?', ':', ';')):
            if word_count < self.ideal_chunk_words:  # S√≥ aceita se for chunk grande
                return False
        
        # Verifica diversidade lexical
        words = re.findall(r'\b\w{2,}\b', text.lower())
        unique_words = set(words)
        if len(words) > 0 and len(unique_words) / len(words) < 0.35:
            return False
        
        return True

    def chunk_structured(self, path: str) -> List[Dict]:
        """Vers√£o otimizada que respeita par√°grafos como unidades sem√¢nticas."""
        # Em vez de carregar por linhas, carregamos o documento inteiro para preservar par√°grafos.
        full_text = self.load_document(path)
        all_chunks = []
        current_page = 1

        # Usa o regex de par√°grafo para dividir o texto em unidades l√≥gicas
        paragraphs = self.paragraph_splitter.split(full_text)
        print(f"üìñ Documento com {len(paragraphs)} par√°grafos detectados.")

        for paragraph_text in paragraphs:
            # Pula par√°grafos vazios ou muito curtos
            if len(paragraph_text.strip()) < self.min_words:
                continue

            # Detecta se o par√°grafo cont√©m uma marca√ß√£o de p√°gina
            page_match = self.page_re.search(paragraph_text)
            if page_match:
                current_page = int(page_match.group(1))
                # Remove a marca√ß√£o de p√°gina do texto para n√£o sujar os chunks
                paragraph_text = self.page_re.sub('', paragraph_text)

            # Detecta se o par√°grafo √© um cabe√ßalho
            if self.header_re.match(paragraph_text.strip()):
                # Trata o cabe√ßalho como um chunk separado e pequeno, se relevante
                # Ou pode ser usado como metadados para os pr√≥ximos chunks
                # Aqui, vamos simplesmente pular para n√£o virar um chunk de texto
                continue

            # Limpa e divide o par√°grafo em senten√ßas
            cleaned_paragraph = self.limpar_texto(paragraph_text)
            sentences = self.split_sentences(cleaned_paragraph)

            if not sentences:
                continue
            
            # Agora, o chunking sem√¢ntico opera DENTRO de um par√°grafo,
            # o que o impede de juntar ideias de par√°grafos diferentes.
            print(f"üîÑ Processando par√°grafo com {len(sentences)} senten√ßas (p√°gina {current_page})")
            
            # A fun√ß√£o chunk_semantic_pairwise √© chamada para cada par√°grafo
            paragraph_chunks = self.chunk_semantic_pairwise(sentences, current_page)
            
            all_chunks.extend(paragraph_chunks)

        # O p√≥s-processamento continua o mesmo
        final_chunks = self._post_process_semantic_chunks(all_chunks)
        
        print(f"üéØ Total: {len(final_chunks)} chunks sem√¢nticos")
        if final_chunks:
            words = [chunk.get('palavras', 0) for chunk in final_chunks]
            print(f"üìè Palavras - M√≠n: {min(words)}, M√°x: {max(words)}, M√©dia: {sum(words)/len(words):.0f}")
        
        return final_chunks

    def _calculate_chunk_cohesion(self, sentences: List[str]) -> float:
        """Calcula a coes√£o interna de um chunk medindo a similaridade m√©dia ao centroide."""
        if not sentences or not ADVANCED_NLP:
            return 0.0
        
        try:
            embeddings = self.model.encode(sentences, show_progress_bar=False)
            # Calcula o centroide (embedding m√©dio) do chunk
            centroid = np.mean(embeddings, axis=0)
            
            # Calcula a similaridade de cada senten√ßa com o centroide
            similarities = cosine_similarity(embeddings, [centroid])
            
            # A coes√£o √© a similaridade m√©dia. Quanto maior, mais coeso √© o chunk.
            return np.mean(similarities)
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao calcular coes√£o do chunk: {e}")
            return 0.0

    def _post_process_semantic_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """P√≥s-processamento focado na qualidade sem√¢ntica, incluindo divis√£o de chunks longos."""
        if not chunks:
            return []
        
        # 1. Limpeza inicial e filtragem b√°sica de chunks recebidos
        #    Garante que 'conteudo', 'palavras', 'sentencas' estejam consistentes.
        pre_processed_candidates = []
        for chunk in chunks:
            content = chunk.get('conteudo', '')
            # Limpeza b√°sica que n√£o deve alterar drasticamente o conte√∫do sem√¢ntico
            content = re.sub(r'\s+', ' ', content).strip()
            content = re.sub(r'^[^\w]*(?=[a-zA-Z0-9√Ä-√ú√†-√º])', '', content) # Remove lixo no in√≠cio, preservando o primeiro caractere √∫til
            
            if not content:
                continue

            # Recalcula senten√ßas e palavras com base no conte√∫do limpo
            current_sentences = self.split_sentences(content)
            if not current_sentences: # Pula se n√£o houver senten√ßas ap√≥s a limpeza
                continue
                
            word_count = self._count_words_in_chunk(current_sentences)

            # Atualiza o chunk com o conte√∫do limpo e contagens recalculadas
            chunk['conteudo'] = content
            chunk['palavras'] = word_count
            chunk['sentencas'] = len(current_sentences)
            
            # Valida√ß√£o sem√¢ntica e de tamanho m√≠nimo
            if self._is_valid_semantic_chunk_words(current_sentences):
                pre_processed_candidates.append(chunk)
            # else:
                # print(f"‚ÑπÔ∏è Chunk descartado na pr√©-valida√ß√£o: {content[:60]}... ({word_count} palavras)")
        
        if not pre_processed_candidates:
            return []

        # 2. Otimiza fronteiras dos chunks (mesclando chunks pequenos adjacentes)
        merged_chunks = self._optimize_chunk_boundaries(pre_processed_candidates)
        if not merged_chunks: # Se a lista estiver vazia ou for None
            return [] 
        
        # Filtra explicitamente quaisquer Nones que possam ter escapado, embora n√£o devessem.
        merged_chunks = [mc for mc in merged_chunks if mc is not None]
        if not merged_chunks: # Se a lista ficar vazia ap√≥s a filtragem
            return []

        
        # 3. Divide chunks que ainda est√£o muito grandes ap√≥s a mesclagem
        #    Esta fun√ß√£o pode alterar o n√∫mero de chunks e suas composi√ß√µes.
        split_oversized_chunks = self._split_all_oversized_chunks(merged_chunks)

        # 4. Processamento final: Atribui√ß√£o de ID sequencial e c√°lculo de 'qualidade'
        final_output_chunks = []
        for i, chunk_data in enumerate(split_oversized_chunks, 1):
            # Revalida cada chunk final, pois as opera√ß√µes anteriores podem ter alterado sua validade
            final_content = chunk_data['conteudo']
            final_sentences = self.split_sentences(final_content) # Re-split para consist√™ncia

            if not final_sentences: # Seguran√ßa extra
                continue

            # Recalcula palavras e senten√ßas para m√°xima precis√£o antes do score de qualidade
            chunk_data['palavras'] = self._count_words_in_chunk(final_sentences)
            chunk_data['sentencas'] = len(final_sentences)

            # S√≥ adiciona se o chunk final ainda for v√°lido e tiver um tamanho m√≠nimo
            if chunk_data['palavras'] >= self.min_chunk_words and \
               self._is_valid_semantic_chunk_words(final_sentences): # _is_valid_semantic_chunk_words j√° checa min_chunk_words
                
                chunk_data['chunk_id'] = i
                chunk_data['qualidade'] = self._calculate_chunk_quality_score(chunk_data) # Recalcula qualidade com dados finais
                final_output_chunks.append(chunk_data)
            # else:
            #     print(f"‚ÑπÔ∏è Chunk descartado no p√≥s-processamento final: {final_content[:60]}... ({chunk_data['palavras']} palavras)")
            
        return final_output_chunks

    def _clean_chunk_text_optimized(self, text: str) -> str:
        """Vers√£o otimizada da limpeza de texto para chunks"""
        if not text:
            return ""
        
        # Cache para evitar processamento repetido
        cache_key = hash(text[:100])  # Use primeiros 100 chars como chave
        if cache_key in self._validation_cache:
            cached_result = self._validation_cache.get(f"clean_{cache_key}")
            if cached_result is not None:
                return cached_result
        
        # Limpeza otimizada em uma passada
        # Remove quebras de linha e normaliza espa√ßos
        text = re.sub(r'\s+', ' ', text)
        
        # Remove caracteres de escape problem√°ticos
        text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')
        text = re.sub(r'\\+', '', text)
        
        # Corrige pontua√ß√£o
        text = re.sub(r'([.!?])\s*\1+', r'\1', text)  # Remove pontua√ß√£o duplicada
        text = re.sub(r'\s+([,.!?;:])', r'\1', text)  # Remove espa√ßos antes de pontua√ß√£o
        text = re.sub(r'([,.!?;:])\s*([,.!?;:])', r'\1 \2', text)  # Espa√ßa pontua√ß√£o m√∫ltipla
        
        # Limpa in√≠cio e fim
        text = re.sub(r'^[^\w\s]*', '', text)  # Remove s√≠mbolos no in√≠cio
        text = re.sub(r'[^\w\s.!?]*$', '', text)  # Remove s√≠mbolos no fim (exceto pontua√ß√£o)
        

        text = text.strip()
        if text and not text[-1] in '.!?:;':

            words = text.split()
            if words and len(words[-1]) >= 3:
                text += '.'
        
        # Cache do resultado
        self._validation_cache[f"clean_{cache_key}"] = text
        
        return text

    def _validate_chunk_content(self, content: str) -> bool:
        """Valida√ß√£o final do conte√∫do do chunk"""
        if not content or len(content.strip()) < 30:
            return False
        
        # Conta palavras reais
        words = re.findall(r'\b\w{2,}\b', content)
        if len(words) < 8:
            return False
        
        # Verifica se n√£o √© s√≥ n√∫meros ou c√≥digos
        if re.match(r'^[\d\s\-\.,]+$', content):
            return False
        
        # Verifica diversidade lexical b√°sica
        unique_words = set(word.lower() for word in words)
        if len(unique_words) < len(words) * 0.3:  # Muito repetitivo
            return False
        
        # Verifica se tem estrutura de frase
        if not re.search(r'\b\w+\s+\w+\b', content):  # Pelo menos duas palavras juntas
            return False
        
        return True

    def _optimize_chunk_boundaries(self, chunks: List[Dict]) -> List[Dict]:
        """Otimiza as fronteiras dos chunks para melhor coes√£o sem√¢ntica"""
        if not chunks or len(chunks) < 2: # Adicionada checagem 'not chunks' para seguran√ßa
            return chunks if chunks is not None else [] # Retorna lista vazia se chunks for None
        
        optimized = []

        temp_chunks_status = [True] * len(chunks) # True se o chunk no √≠ndice i ainda precisa ser processado

        for i, current_chunk_original_ref in enumerate(chunks):
            if not temp_chunks_status[i]: # Se o chunk atual j√° foi mesclado com um anterior
                continue

            chunk = current_chunk_original_ref 


            content = chunk['conteudo'] # Se chunk fosse None, esta linha falharia.
                                        # Mas com temp_chunks_status, n√£o dever√≠amos pegar um "consumido".

            # Usar .get() para 'palavras' para evitar KeyError se a chave n√£o existir por algum motivo.
            word_count = chunk.get('palavras', len(content.split())) 


            if word_count < self.min_chunk_words and i < (len(chunks) - 1) and temp_chunks_status[i+1]:
                next_chunk_original_ref = chunks[i+1]

                combined_content_text = self._clean_chunk_text_optimized(content + ' ' + next_chunk_original_ref['conteudo'])
                combined_sentences = self.split_sentences(combined_content_text)
                combined_words = self._count_words_in_chunk(combined_sentences)

                if combined_words <= self.max_chunk_words * 1.2: # Limite de mesclagem
                    merged_chunk = chunk.copy() 
                    merged_chunk['conteudo'] = combined_content_text
                    merged_chunk['palavras'] = combined_words
                    merged_chunk['sentencas'] = len(combined_sentences) 
                    merged_chunk['tipo_quebra'] = 'mesclado'
                    # Calcula a confian√ßa da mesclagem
                    conf_chunk = chunk.get('confianca', 0.8)
                    conf_next_chunk = next_chunk_original_ref.get('confianca', 0.8)
                    merged_chunk['confianca'] = float(min(conf_chunk, conf_next_chunk))
                    
                    optimized.append(merged_chunk)
                    temp_chunks_status[i] = False # Marca o chunk atual como processado (mesclado)
                    temp_chunks_status[i+1] = False # Marca o pr√≥ximo chunk como processado (mesclado)
                    continue # Pula a adi√ß√£o individual do chunk atual
            
            # Se n√£o mesclou (ou era o √∫ltimo, ou o pr√≥ximo j√° foi processado, ou a mescla excedeu o tamanho)
            optimized.append(chunk)
            temp_chunks_status[i] = False # Marca como processado (adicionado individualmente)
            
        return optimized # 'optimized' n√£o deve conter 'None' com esta l√≥gica

    def _calculate_chunk_quality_score(self, chunk: Dict) -> float:
        """Calcula score de qualidade do chunk"""
        content = chunk.get('conteudo', '')
        if not content:
            return 0.0
        
        score = 1.0
        words = content.split()
        word_count = len(words)
        
        # Penaliza chunks muito pequenos ou muito grandes
        if word_count < self.min_chunk_words:
            score *= 0.5
        elif word_count > self.max_chunk_words:
            score *= 0.8
        
        # Bonifica chunks no tamanho ideal
        if self.min_chunk_words <= word_count <= self.ideal_chunk_words:
            score *= 1.1
        
        # Verifica termina√ß√£o adequada
        if content.rstrip().endswith(('.', '!', '?')):
            score *= 1.05
        elif content.rstrip().endswith((':', ';')):
            score *= 1.02
        else:
            score *= 0.9
        
        # Verifica diversidade lexical
        unique_words = set(word.lower() for word in words if len(word) > 2)
        if words:
            diversity = len(unique_words) / len(words)
            score *= (0.7 + diversity * 0.6)  # Score entre 0.7 e 1.3
        
        # Usa confian√ßa existing se dispon√≠vel
        existing_confidence = chunk.get('confianca', 0.8)
        score = (score + existing_confidence) / 2
        
        return float(min(score, 1.0))


    def _try_split_single_oversized_chunk(self, oversized_chunk: Dict) -> List[Dict]:
        """
        Tenta dividir um chunk semanticamente se ele for muito longo.
        Retorna uma lista de chunks (1 se n√£o dividido, 2+ se dividido).
        """
        content = oversized_chunk['conteudo']
        original_page = oversized_chunk.get('pagina')
        # Use as contagens originais como fallback se a divis√£o falhar
        original_palavras = oversized_chunk['palavras']
        original_sentencas = oversized_chunk['sentencas']

        internal_sentences = self.split_sentences(content)

   
        if len(internal_sentences) < self.min_sentences_per_chunk * 2:
            return [oversized_chunk]

     
        similarities = self._calculate_semantic_similarity_batch(internal_sentences)
        if not similarities: 
            return [oversized_chunk]

        best_split_point_idx = -1 
        lowest_similarity_at_split = 1.0

        for i in range(len(similarities)):
            num_sentences_chunk1 = i + 1
            num_sentences_chunk2 = len(internal_sentences) - num_sentences_chunk1

            if num_sentences_chunk1 < self.min_sentences_per_chunk or \
               num_sentences_chunk2 < self.min_sentences_per_chunk:
                continue

            sentences1 = internal_sentences[:num_sentences_chunk1]
            sentences2 = internal_sentences[num_sentences_chunk1:]

            words1 = self._count_words_in_chunk(sentences1)
            words2 = self._count_words_in_chunk(sentences2)

            if not (self.min_chunk_words <= words1 <= self.max_chunk_words and
                    words2 >= self.min_chunk_words):
                continue
            current_similarity = similarities[i]
            
            split_similarity_threshold = self.threshold_base - 0.15 

            if current_similarity < split_similarity_threshold:
                if current_similarity < lowest_similarity_at_split:
                    lowest_similarity_at_split = current_similarity
                    best_split_point_idx = i
        
        if best_split_point_idx != -1:
            split_after_sentence_index = best_split_point_idx
            chunk1_sentences = internal_sentences[:split_after_sentence_index + 1]
            chunk2_sentences = internal_sentences[split_after_sentence_index + 1:]

            chunk1_text = self._clean_chunk_text_optimized(' '.join(chunk1_sentences))
            chunk2_text = self._clean_chunk_text_optimized(' '.join(chunk2_sentences))
            
            # Revalida os novos chunks
            if not self._is_valid_semantic_chunk_words(chunk1_sentences) or \
               not self._is_valid_semantic_chunk_words(chunk2_sentences):
                return [oversized_chunk] # Divis√£o resultou em chunks inv√°lidos

            palavras_c1 = self._count_words_in_chunk(chunk1_sentences)
            palavras_c2 = self._count_words_in_chunk(chunk2_sentences)

            new_chunks = [
                {
                    'pagina': original_page,
                    'conteudo': chunk1_text,
                    'tipo_quebra': 'split_long', # Indica que este chunk resultou de uma divis√£o
                    'confianca': float(max(0.6, min(0.95, 1.0 - lowest_similarity_at_split))), # Confian√ßa baseada na dissimilaridade
                    'palavras': palavras_c1,
                    'sentencas': len(chunk1_sentences)
                },
                {
                    'pagina': original_page,
                    'conteudo': chunk2_text,
                    'tipo_quebra': 'split_long_cont', # Continua√ß√£o do chunk dividido
                    'confianca': float(max(0.6, min(0.95, 1.0 - lowest_similarity_at_split))),
                    'palavras': palavras_c2,
                    'sentencas': len(chunk2_sentences)
                }
            ]
            print(f"‚úÇÔ∏è Chunk longo (P:{original_page}, {original_palavras}p/{original_sentencas}s) dividido em 2. "
                  f"Novos: ({new_chunks[0]['palavras']}p/{new_chunks[0]['sentencas']}s) e ({new_chunks[1]['palavras']}p/{new_chunks[1]['sentencas']}s). "
                  f"Similaridade no split: {lowest_similarity_at_split:.2f}")
            return new_chunks
        
        return [oversized_chunk] # N√£o foi poss√≠vel dividir ou encontrar um bom ponto de divis√£o

    def _split_all_oversized_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """
        Itera sobre os chunks e tenta dividir aqueles que excedem max_chunk_words.
        Repete o processo se houver divis√µes, at√© um limite de itera√ß√µes.
        """
        trigger_word_count = self.max_chunk_words 
        # Poderia usar um valor um pouco maior que self.max_chunk_words para dar uma margem, 
        # ex: self.max_chunk_words * 1.1, mas para atender ao pedido de quebrar os "muito longos",
        # usar self.max_chunk_words diretamente √© mais alinhado.

        processed_chunks_in_iteration = True 
        current_chunks = list(chunks) 

        max_iterations = 5 # Para evitar loops infinitos em casos complexos
        num_iterations = 0

        while processed_chunks_in_iteration and num_iterations < max_iterations:
            processed_chunks_in_iteration = False
            num_iterations += 1
            
            next_iteration_chunks = []
            for chunk_item in current_chunks:
                # Verifica se o chunk atual precisa ser avaliado para divis√£o
                if chunk_item['palavras'] > trigger_word_count:
                    split_attempt_results = self._try_split_single_oversized_chunk(chunk_item)
                    next_iteration_chunks.extend(split_attempt_results)
                    if len(split_attempt_results) > 1: # Se o chunk foi efetivamente dividido
                        processed_chunks_in_iteration = True 
                else:
                    next_iteration_chunks.append(chunk_item) # Mant√©m o chunk como est√°
            current_chunks = next_iteration_chunks
            
            if processed_chunks_in_iteration:
                 print(f"üîÑ Repassando divis√£o de chunks (Itera√ß√£o {num_iterations}). Total atual: {len(current_chunks)} chunks.")

        return current_chunks



    def is_valid_content(self, content: str) -> bool:
        """Valida√ß√£o mais rigorosa com m√∫ltiplos crit√©rios"""
        if not content or len(content) < self.min_words:
            return False
            
        # Verifica padr√µes inv√°lidos
        invalid_patterns = [
            r'^\d+$',  # Apenas n√∫meros
            r'^[^a-zA-Z0-9]{5,}$',  # S√≠mbolos
            r'^\s*[A-Z]{2,}\s*$'  # Siglas
        ]
        
        if any(re.match(p, content) for p in invalid_patterns):
            return False
            
        # Verifica√ß√£o gramatical com fallback
        return self.tem_sentido(content) if len(content) < 120 else True

    def chunk_faq(self, lines: List[str]) -> List[Dict]:
        """Processa FAQ format"""
        chunks = []
        chunk_id = 1
        
        for i in range(len(lines) - 1):
            question_match = self.faq_q_re.match(lines[i])  # Nome correto
            answer_match = self.faq_a_re.match(lines[i + 1])  # Nome correto
            
            if question_match and answer_match:
                question = question_match.group(1).strip()
                answer = answer_match.group(1).strip()
                
                if question and answer:  # Valida se tem conte√∫do
                    chunks.append({
                        'chunk_id': chunk_id,
                        'pergunta': question,
                        'resposta': answer
                    })
                    chunk_id += 1
        
        return chunks

    def parse_interview_blocks(self, lines: List[str]) -> List[tuple]:
        """Parse blocos de entrevista/di√°logo"""
        blocks = []
        current_block = None
        current_header = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Detecta cabe√ßalho
            if self.header_interview_re.match(line):
                current_header = line
                continue
            
            # Detecta speaker
            speaker_match = self.speaker_re.match(line)
            if speaker_match:
                name, text = speaker_match.groups()
                name, text = name.strip(), text.strip()
                
                if current_block and current_block[0] == name:
                    current_block[1].append(text)
                else:
                    if current_block:
                        blocks.append((current_header, current_block))
                    current_block = [name, [text]]
        
        if current_block:
            blocks.append((current_header, current_block))
        
        return blocks

    def chunk_interview(self, blocks: List[tuple], has_header: bool = False) -> List[Dict]:
        """Processa formato de entrevista"""
        chunks = []
        chunk_id = 1
        
        # Processa pares de blocos (pergunta-resposta)
        for i in range(0, len(blocks) - 1, 2):
            header_a, block_a = blocks[i]
            header_b, block_b = blocks[i + 1]
            
            pergunta = f"{block_a[0]}: {' '.join(block_a[1])}"
            resposta = f"{block_b[0]}: {' '.join(block_b[1])}"
            
            chunk = {
                'chunk_id': chunk_id,
                'pergunta': pergunta,
                'resposta': resposta
            }
            
            if has_header and header_a:
                chunk['cabecalho'] = header_a
            
            chunks.append(chunk)
            chunk_id += 1
        
        return chunks

    def clear_cache(self):
        """Limpa caches para liberar mem√≥ria"""
        self._validation_cache.clear()
        self._embedding_cache.clear()

    def chunk_semantic_v3_enhanced(self, path: str, progress_queue: Queue = None) -> List[Dict]:
        """Wrapper para compatibilidade com o sistema de progresso"""
        return self.chunk_structured(path)

    def _calculate_semantic_similarity_batch(self, sentences: List[str]) -> List[float]:
        """
        Calcula a similaridade sem√¢ntica entre senten√ßas consecutivas em lote.
        """
        if len(sentences) < 2:
            return []

        # Garante que a biblioteca necess√°ria est√° dispon√≠vel
        if not ADVANCED_NLP:
            print("AVISO: An√°lise de similaridade requer bibliotecas avan√ßadas.")
            # Retorna um valor padr√£o neutro para evitar quebra total
            return [0.8] * (len(sentences) - 1)

        try:
            # Usa o modelo principal para gerar os embeddings
            embeddings = self.model.encode(sentences, batch_size=self.batch_size, show_progress_bar=False)
            
            # Calcula a similaridade de cosseno entre pares de senten√ßas adjacentes
            # Compara o embedding[i] com o embedding[i+1]
            sims = cosine_similarity(
                embeddings[:-1],
                embeddings[1:]
            )
            
            # A similaridade entre a senten√ßa 'i' e 'i+1' est√° na diagonal da matriz
            return [sims[i][i] for i in range(len(sims))]

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao calcular similaridade sem√¢ntica: {e}")
            return [0.0] * (len(sentences) - 1)


class Application:
    def __init__(self, master):
        self.master = master
        self.processor = DocumentProcessor()
        self.json_data = None
        self.setup_ui()
        
    def setup_processor_with_api_key(self, api_key: str = None):
        """Configura o processador com chave da API"""
        self.processor = DocumentProcessor(gemini_api_key=api_key)

    def setup_ui(self):
        style = ttk.Style()
        style.theme_use('clam')
        
        # Cores do tema dark (mantidas as mesmas)
        dark_bg = '#23272e'
        dark_panel = '#2c313a'
        accent = '#5e81ac'
        accent_hover = '#81a1c1'
        text_color = '#e5e9f0'
        label_color = '#bfc9db'
        entry_bg = '#181a20'
        border_color = '#444857'
        card_bg = '#252a33'
        card_border = '#3b4252'
        shadow = '#181a20'

        # Configura√ß√£o de estilos (mantida igual)
        style.configure('TFrame', background=dark_bg)
        style.configure('TLabel', background=dark_bg, foreground=label_color, font=('Segoe UI', 11))
        style.configure('Title.TLabel', background=dark_bg, foreground=accent, font=('Segoe UI', 20, 'bold'))
        style.configure('Subtitle.TLabel', background=dark_bg, foreground=label_color, font=('Segoe UI', 12, 'italic'))
        style.configure('TButton', font=('Segoe UI', 12, 'bold'), padding=10, background=accent, foreground=text_color, borderwidth=0)
        style.map('TButton', background=[('active', accent_hover)], foreground=[('active', text_color)])
        style.configure('TCheckbutton', background=dark_bg, foreground=label_color, font=('Segoe UI', 10))
        style.configure('TEntry', fieldbackground=entry_bg, foreground=text_color, bordercolor=border_color, font=('Segoe UI', 12))
        style.configure('TCombobox', fieldbackground=entry_bg, foreground=text_color, background=entry_bg, bordercolor=border_color, font=('Segoe UI', 12))
        style.map('TCombobox', fieldbackground=[('readonly', entry_bg)], foreground=[('readonly', text_color)])

        self.master.title("Processador de Documentos - Vers√£o Melhorada")
        self.master.geometry("950x750")
        self.master.configure(bg=dark_bg)

        # Interface (mantida similar, mas com melhorias)
        title = ttk.Label(self.master, text="Processador de Documentos", style='Title.TLabel')
        title.pack(pady=(18, 2))
        subtitle = ttk.Label(self.master, text="Chunking sem√¢ntico inteligente com valida√ß√£o de conte√∫do", style='Subtitle.TLabel')
        subtitle.pack(pady=(0, 12))

        # Frame principal
        main_frame = ttk.Frame(self.master, padding=24, style='TFrame')
        main_frame.pack(fill=X, padx=40, pady=(0, 10))

        # Linha do arquivo
        file_frame = ttk.Frame(main_frame, style='TFrame')
        file_frame.pack(fill=X, pady=10)
        ttk.Label(file_frame, text="Arquivo:").pack(side=LEFT, padx=(0, 8))
        self.file_entry = ttk.Entry(file_frame, width=48)
        self.file_entry.pack(side=LEFT, padx=(0, 8))
        btn_browse = ttk.Button(file_frame, text="üîç Procurar", command=self.browse_file)
        btn_browse.pack(side=LEFT)
        download_btn = ttk.Button(file_frame, text="‚¨áÔ∏è Baixar JSON", command=self.save_json)
        download_btn.pack(side=LEFT, padx=(12, 0))

        # ==== Campo para API Key do Gemini ====
        api_frame = ttk.Frame(main_frame, style='TFrame')
        api_frame.pack(fill=X, pady=10)
        ttk.Label(api_frame, text="API Key Gemini:").pack(side=LEFT, padx=(0, 8))
        self.api_key_entry = ttk.Entry(api_frame, width=40, show="*")
        self.api_key_entry.pack(side=LEFT, padx=(0, 8))
        btn_config_api = ttk.Button(api_frame, text="üîë Configurar", command=self.configure_gemini_api)
        btn_config_api.pack(side=LEFT)
        self.api_status_label = ttk.Label(api_frame, text="‚ùå Gemini n√£o configurado", 
                                         font=('Segoe UI', 9, 'italic'))
        self.api_status_label.pack(side=LEFT, padx=(12, 0))

        # Configura√ß√µes avan√ßadas
        config_frame = ttk.Frame(main_frame, style='TFrame')
        config_frame.pack(fill=X, pady=10)
        
        ttk.Label(config_frame, text="Estilo:").pack(side=LEFT, padx=(0, 8))
        self.doc_type = ttk.Combobox(config_frame, values=["FAQ", "Pergunta-Resposta/Entrevista", "Texto Puro"], 
                                   state="readonly", width=24)
        self.doc_type.pack(side=LEFT, padx=(0, 8))
        self.doc_type.bind("<<ComboboxSelected>>", self.toggle_header_check)
        
        self.header_var = IntVar()
        self.header_check = ttk.Checkbutton(config_frame, text="Cont√©m cabe√ßalhos", variable=self.header_var)
        
        # Controles de threshold
        ttk.Label(config_frame, text="Similaridade:").pack(side=LEFT, padx=(15, 5))
        self.threshold_var = DoubleVar(value=0.75)
        threshold_scale = ttk.Scale(config_frame, from_=0.5, to=0.9, variable=self.threshold_var, 
                                  orient=HORIZONTAL, length=100)
        threshold_scale.pack(side=LEFT, padx=(0, 5))
        self.threshold_label = ttk.Label(config_frame, text="0.75")
        self.threshold_label.pack(side=LEFT)
        threshold_scale.configure(command=self.update_threshold_label)

        # Status e progresso
        self.status_var = StringVar(value="Pronto para processar")
        status_label = ttk.Label(main_frame, textvariable=self.status_var, 
                               font=('Segoe UI', 10, 'italic'))
        status_label.pack(pady=(10, 0))

        # Card de exemplo (mantido)
        card_frame = Frame(main_frame, bg=card_bg, highlightbackground=card_border, highlightthickness=1, bd=0)
        card_frame.pack(fill=X, pady=(10, 16), padx=2)
        self.example_label = Label(card_frame, text="", font=("Segoe UI", 11, "italic"), 
                                 fg=accent, bg=card_bg, justify=LEFT, anchor=W, padx=12, pady=8)
        self.example_label.pack(fill=X, anchor=W)
        self.update_example_label()

        # Bot√£o de processar
        self.process_btn = ttk.Button(main_frame, text="‚ö° Processar Arquivo", command=self.process_file)
        self.process_btn.pack(pady=(0, 10), ipadx=10)

        # Frame de visualiza√ß√£o
        view_frame = Frame(self.master, bg=shadow)
        view_frame.pack(fill=BOTH, expand=1, padx=40, pady=(0, 10))
        result_card = Frame(view_frame, bg=card_bg, highlightbackground=card_border, highlightthickness=2, bd=0)
        result_card.pack(fill=BOTH, expand=1, padx=0, pady=0)
        
        # Header do resultado
        result_header = ttk.Frame(result_card, style='TFrame')
        result_header.configure(style='Card.TFrame')
        result_header.pack(fill=X, pady=(8, 0), padx=12)
        
        ttk.Label(result_header, text="Resultado:", font=("Segoe UI", 12, "bold"), 
                foreground=accent, background=card_bg).pack(side=LEFT)
        
        self.chunk_count_label = ttk.Label(result_header, text="", font=("Segoe UI", 10), 
                                         foreground=label_color, background=card_bg)
        self.chunk_count_label.pack(side=RIGHT)
        
        # √Årea de texto
        text_frame = Frame(result_card, bg=card_bg)
        text_frame.pack(fill=BOTH, expand=1, padx=12, pady=(5, 12))
        
        self.json_text = Text(text_frame, wrap=NONE, font=("Consolas", 11), 
                            bg=dark_panel, fg=text_color, insertbackground=accent, 
                            relief=GROOVE, borderwidth=2, height=18)
        self.json_text.pack(side=LEFT, fill=BOTH, expand=1)
        
        scroll_y = ttk.Scrollbar(text_frame, command=self.json_text.yview)
        scroll_y.pack(side=RIGHT, fill=Y)
        self.json_text.configure(yscrollcommand=scroll_y.set)

        # --- Barra de progresso ---
        self.progress_bar = ttk.Progressbar(main_frame, orient='horizontal', length=300, mode='determinate')
        self.progress_bar.pack(pady=(5, 10), fill=X, padx=50)
        self.progress_bar.pack_forget()  # Esconde inicialmente

    def update_threshold_label(self, value):
        """Atualiza label do threshold"""
        self.threshold_label.config(text=f"{float(value):.2f}")
        self.processor.threshold = float(value)

    def toggle_header_check(self, event=None):
        """Toggle checkbox de cabe√ßalho"""
        if self.doc_type.get() == "Pergunta-Resposta/Entrevista":
            self.header_check.pack(side=LEFT, padx=(8, 0))
        else:
            self.header_check.pack_forget()
        self.update_example_label()

    def update_example_label(self):
        """Atualiza exemplo baseado no tipo selecionado"""
        estilo = self.doc_type.get()
        if estilo == "FAQ":
            exemplo = "üí° Exemplo:\n1. Qual √© a capital do Brasil?\nR: Bras√≠lia √© a capital."
        elif estilo == "Pergunta-Resposta/Entrevista":
            exemplo = "üí° Exemplo:\nEntrevistador: Como voc√™ come√ßou?\nEntrevistado: Comecei h√° 5 anos..."
        else:
            exemplo = "üí° Exemplo:\nTexto livre ser√° dividido em chunks sem√¢nticos"
            if hasattr(self, 'processor') and self.processor and self.processor.use_gemini:
                exemplo += " usando IA Gemini para m√°xima precis√£o! ü§ñ"
            else:
                exemplo += " usando embeddings tradicionais."
        self.example_label.config(text=exemplo)

    def browse_file(self):
        """Seleciona arquivo"""
        filepath = filedialog.askopenfilename(
            title="Selecionar documento",
            filetypes=[
                ("Todos os suportados", "*.txt *.pdf *.docx"),
                ("Arquivos de texto", "*.txt"),
                ("PDFs", "*.pdf"),
                ("Word", "*.docx")
            ]
        )
        if filepath:
            self.file_entry.delete(0, END)
            self.file_entry.insert(0, filepath)
            self.status_var.set(f"Arquivo selecionado: {os.path.basename(filepath)}")

    def process_file(self):
        """Inicia o processamento em thread separada"""
        path = self.file_entry.get().strip()
        if not path or not os.path.isfile(path):
            messagebox.showerror("Erro", "Selecione um caminho de arquivo v√°lido!")
            return
        if not self.processor:
            messagebox.showerror("Erro", "O processador de documentos n√£o foi inicializado.")
            return
        # Prepara a UI
        self.process_btn.config(state="disabled")
        self.status_var.set("Iniciando processamento...")
        self.json_text.delete(1.0, END)
        self.chunk_count_label.config(text="")
        self.progress_bar.pack()
        self.progress_bar['value'] = 0
        self.master.update_idletasks()
        # Cria queue e thread
        self.result_queue = Queue()
        doc_type = self.doc_type.get()
        has_header = self.header_var.get() == 1 if doc_type == "Pergunta-Resposta/Entrevista" else False
        threading.Thread(
            target=self._background_process_file,
            args=(path, doc_type, has_header),
            daemon=True
        ).start()
        # Inicia verifica√ß√£o da queue
        self.master.after(100, self._check_processing_queue)

    def _background_process_file(self, path, doc_type, has_header):
        """Processamento em background"""
        try:
            if doc_type == "FAQ":
                # Passe is_faq_mode=True para o load_lines
                lines = self.processor.load_lines(path, is_faq_mode=True) 
                # Opcional: Adicione um print para depura√ß√£o
                # print(f"DEBUG: Linhas para FAQ (total {len(lines)}):")
                # for i, l in enumerate(lines):
                #     print(f"  {i}: {l}")
                chunks = self.processor.chunk_faq(lines)
                self.result_queue.put({"type": "result", "success": True, "data": chunks})
            elif doc_type == "Pergunta-Resposta/Entrevista":
                # Para outros tipos, chame load_lines sem is_faq_mode (ou com False)
                lines = self.processor.load_lines(path) 
                blocks = self.processor.parse_interview_blocks(lines)
                chunks = self.processor.chunk_interview(blocks, has_header)
                self.result_queue.put({"type": "result", "success": True, "data": chunks})
            else:  # Texto Puro
                # chunk_structured n√£o usa load_lines diretamente da mesma forma,
                # ele usa load_document. N√£o precisa de is_faq_mode aqui.
                chunks = self.processor.chunk_structured(path)
                self.result_queue.put({"type": "result", "success": True, "data": chunks})
        except Exception as e:
            # print(f"DEBUG: Erro em _background_process_file: {e}") # Para depura√ß√£o
            self.result_queue.put({"type": "result", "success": False, "error": str(e)})

    def _check_processing_queue(self):
        """Verifica atualiza√ß√µes na queue"""
        try:
            while not self.result_queue.empty():
                msg = self.result_queue.get_nowait()
                if msg['type'] == 'progress':
                    self.progress_bar['value'] = msg['value']
                    self.status_var.set(msg['step'])
                    self.master.update_idletasks()
                elif msg['type'] == 'result':
                    self.progress_bar.pack_forget()
                    self.process_btn.config(state="normal")
                    if msg['success']:
                        chunks = msg['data']
                        self.json_data = json.dumps(chunks, ensure_ascii=False, indent=2)
                        self.json_text.insert(END, self.json_data)
                        self.chunk_count_label.config(text=f"{len(chunks)} chunks")
                        messagebox.showinfo("Sucesso", f"Processamento conclu√≠do!\n{len(chunks)} chunks gerados.")
                    else:
                        messagebox.showerror("Erro", f"Falha no processamento:\n{msg['error']}")
                    return
            self.master.after(100, self._check_processing_queue)
        except Exception as e:
            logging.error(f"Erro na queue: {str(e)}")
            self.progress_bar.pack_forget()
            self.process_btn.config(state="normal")

    def save_json(self):
        """Salva o JSON processado"""
        if not self.json_data:
            messagebox.showwarning("Aviso", "Nenhum dado processado para salvar!")
            return

        entrada = self.file_entry.get()
        if entrada:
            base = os.path.splitext(os.path.basename(entrada))[0]
            sugestao = f"{base}_chunks.json"
        else:
            sugestao = "chunks.json"

        filepath = filedialog.asksaveasfilename(
            title="Salvar chunks como JSON",
            defaultextension=".json",
            initialfile=sugestao,
            filetypes=[("JSON files", "*.json"), ("Todos os arquivos", "*.")]
        )
        
        if filepath:
            try:
                with open(filepath, 'w', encoding='utf-8') as f:
                    try:
                        json.loads(self.json_data)  # Valida o JSON
                        f.write(self.json_data)
                    except Exception as e:
                        messagebox.showerror("Erro", f"JSON inv√°lido: {str(e)}")
                        return
                messagebox.showinfo("Sucesso", f"Arquivo salvo em:\n{filepath}")
                self.status_var.set(f"Salvo: {os.path.basename(filepath)}")
            except Exception as e:
                messagebox.showerror("Erro", f"Erro ao salvar arquivo:\n{str(e)}")

    def configure_gemini_api(self):
        """Configura a API do Gemini de forma n√£o-bloqueante"""
        api_key = self.api_key_entry.get().strip()
        if not api_key:
            messagebox.showwarning("Aviso", "Digite uma chave API v√°lida!")
            return
        
        # Desabilita controles durante teste
        self.api_key_entry.config(state="disabled")
        self.api_status_label.config(text="üîÑ Testando...", foreground='#d08770')
        self.master.update_idletasks()
        
        def test_api():
            try:
                genai.configure(api_key=api_key)
                # Teste mais r√°pido e simples
                test_model = genai.GenerativeModel('gemini-1.5-flash')
                test_response = test_model.generate_content("Responda apenas: OK")
                
                # Configura processador
                self.setup_processor_with_api_key(api_key)
                
                # Atualiza UI na thread principal
                self.master.after(0, lambda: self._api_success())
                
            except Exception as e:
                error_msg = str(e)
                self.master.after(0, lambda: self._api_error(error_msg))
        
        # Executa teste em thread separada
        threading.Thread(target=test_api, daemon=True).start()

    def _api_success(self):
        """Callback para sucesso na API"""
        self.api_status_label.config(text="‚úÖ Gemini ativo", foreground='#5e81ac')
        self.api_key_entry.config(state="normal")
        messagebox.showinfo("Sucesso", "ü§ñ Gemini configurado!\nChunking sem√¢ntico com IA ativado.")
        self.update_example_label()  # Atualiza exemplo

    def _api_error(self, error_msg):
        """Callback para erro na API"""
        self.api_status_label.config(text="‚ùå Erro na chave", foreground='#bf616a')
        self.api_key_entry.config(state="normal")
        messagebox.showerror("Erro", f"Falha ao configurar Gemini:\n{error_msg}\n\nVerifique a chave e cr√©ditos.")


def main():
    """Fun√ß√£o principal"""
    # Configura logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Suprime logs verbosos do pdfminer
    logging.getLogger("pdfminer").setLevel(logging.ERROR)
    
    # Cria e executa aplica√ß√£o
    root = Tk()
    app = Application(root)
    
    try:
        root.mainloop()
    except KeyboardInterrupt:
        print("Aplica√ß√£o interrompida pelo usu√°rio")
    except Exception as e:
        logging.error(f"Erro fatal na aplica√ß√£o: {e}")
        messagebox.showerror("Erro Fatal", f"Erro inesperado:\n{str(e)}")


if __name__ == '__main__':
    main()
